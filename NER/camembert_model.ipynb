{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, AutoTokenizer, CamembertForTokenClassification, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset, load_metric, load_dataset, ClassLabel, DownloadConfig\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizer import Tokenizer\n",
    "from trajet_dataset import IOBTRAJETDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(p, label_list):\n",
    "#     predictions, labels = p\n",
    "#     predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "#     # Remove ignored index (special tokens)\n",
    "#     true_predictions = [\n",
    "#         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "#     true_labels = [\n",
    "#         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "\n",
    "#     results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "#     return {\n",
    "#         \"precision\": results[\"overall_precision\"],\n",
    "#         \"recall\": results[\"overall_recall\"],\n",
    "#         \"f1\": results[\"overall_f1\"],\n",
    "#         \"accuracy\": results[\"overall_accuracy\"],\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "\n",
    "def compute_metrics(p):\n",
    "    if \"loss\" in p:\n",
    "        loss = p[\"loss\"].item()\n",
    "    else:\n",
    "        predictions = np.argmax(p.predictions, axis=-1)\n",
    "        labels = p.label_ids\n",
    "        confusion_matrices = multilabel_confusion_matrix(labels.flatten(), predictions.flatten())\n",
    "        true_positives = confusion_matrices[:, 1, 1]\n",
    "        false_positives = confusion_matrices[:, 0, 1]\n",
    "        false_negatives = confusion_matrices[:, 1, 0]\n",
    "\n",
    "        precision = true_positives / np.maximum((true_positives + false_positives), 1)\n",
    "        recall = true_positives / np.maximum((true_positives + false_negatives), 1)\n",
    "        f1 = 2 * (precision * recall) / np.maximum((precision + recall), 1)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "    return {\n",
    "        'precision': np.mean(precision),\n",
    "        'recall': np.mean(recall),\n",
    "        'f1': np.mean(f1),\n",
    "        'loss': loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n_version = \"trajet_v2\"\n",
    "max_epochs = 6\n",
    "learning_rate = 2e-5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_checkpoint = \"camembert-base\"\n",
    "pretrained_tokenizer_checkpoint = \"camembert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 14:02:09,745 - INFO - SpaCy model loaded successfully.\n",
      "2024-02-09 14:02:09,745 - INFO - Generating sentences...\n",
      "2024-02-09 14:02:48,960 - INFO - Processing sentences...\n",
      "2024-02-09 14:20:27,409 - INFO - Saving data to datas/train_iob.csv...\n",
      "2024-02-09 14:20:45,436 - INFO - Saving data to datas/dev_iob.csv...\n",
      "2024-02-09 14:20:58,386 - INFO - Saving data to datas/test_iob.csv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping span (does not align to tokens): 0 3 DEP Lor\n",
      "Skipping span (does not align to tokens): 45 54 DEST Plourac'h\n",
      "Skipping span (does not align to tokens): 47 57 DEST Kermoroc'h\n",
      "Skipping span (does not align to tokens): 48 58 DEST Kermoroc'h\n",
      "Skipping span (does not align to tokens): 36 50 DEST Guilligomarc'h\n",
      "Skipping span (does not align to tokens): 86 96 DEST Kermoroc'h\n",
      "Skipping span (does not align to tokens): 55 64 DEST Ploulec'h\n",
      "Skipping span (does not align to tokens): 48 57 DEST Ploulec'h\n"
     ]
    }
   ],
   "source": [
    "# !python generate_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IOBTRAJETDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Tokenizer.init_vf(pretrained_tokenizer_checkpoint=pretrained_tokenizer_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CamembertForTokenClassification.from_pretrained(pretrained_model_checkpoint,\n",
    "                                                            num_labels=len(dataset.labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-DEP', 1: 'B-DEST', 2: 'I-DEP', 3: 'I-DEST', 4: 'O'}\n",
      "{'B-DEP': 0, 'B-DEST': 1, 'I-DEP': 2, 'I-DEST': 3, 'O': 4}\n"
     ]
    }
   ],
   "source": [
    "model.config.id2label = dataset.id2label\n",
    "\n",
    "print(model.config.id2label)\n",
    "model.config.label2id = dataset.label2id\n",
    "print(model.config.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  76%|███████▋  | 91000/119185 [00:51<00:15, 1803.96 examples/s]"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.dataset.map(preprocessor.tokenize_and_align_labels, \n",
    "                                         batched=True,\n",
    "                                         load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        f\"test-ner\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=max_epochs,\n",
    "        weight_decay=0.01, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=preprocessor.tokenizer, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=preprocessor.tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p=p)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5592 [00:00<?, ?it/s]You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 4/5592 [14:41<341:27:22, 219.98s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_predictions = [\n",
    "        [dataset.labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "true_labels = [\n",
    "            [dataset.labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "print(\"*\" *200)\n",
    "print(results)\n",
    "print(\"*\" *200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the 'results' directory exists\n",
    "results_directory = \"results\"\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "\n",
    "# Specify the file path\n",
    "results_file_path = os.path.join(results_directory, \"results.json\")\n",
    "\n",
    "# Define a custom encoder function to handle int32 objects\n",
    "def custom_encoder(obj):\n",
    "    if isinstance(obj, np.int32):\n",
    "        return int(obj)\n",
    "    raise TypeError(\"Object not serializable\")\n",
    "\n",
    "# Save results to a JSON file with the custom encoder\n",
    "with open(results_file_path, 'w') as results_file:\n",
    "    json.dump(results, results_file, default=custom_encoder)\n",
    "\n",
    "print(f\"Results saved to: {results_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_dir = os.path.expanduser(\"models\") + \"/\" + model_n_version #models\\trajet_v1\n",
    "trainer.save_model(out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
