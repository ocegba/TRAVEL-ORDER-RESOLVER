{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entité\n",
    "élement qui est spécifique et identifiable dans un texte dans le contexte NER\n",
    "fait référence à des objets du monde réel tels que des personnes, des lieux, des organisations, des dates, des montants monétaires, etc. \n",
    "\n",
    "sont des éléments importants dans un texte, car elles portent des informations clés et contribuent à la compréhension globale du contenu.\n",
    "\n",
    "- Personnes : \"Barack Obama\", \"John Smith\"\n",
    "- Lieux : \"Paris\", \"New York\", \"Mont Everest\"\n",
    "- Organisations : \"Google\", \"United Nations\", \"Apple Inc.\"\n",
    "- Dates : \"10 août 2023\", \"1er janvier 2000\"\n",
    "- Montants monétaires : \"100 dollars\", \"1 000 euros\"\n",
    "- Noms de produits : \"iPhone 13\", \"Coca-Cola\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER (Named-entity recognition)\n",
    "\n",
    "technique d'extraction d'infos qui vise à identifier et classifier les entités nommées dans un txt.\n",
    "\n",
    "ces entités peuvent être :\n",
    "- prédéfinies ou géneriques : lieux, organisations, indiations temporelles\n",
    "- ou assez spécifiques : par ex dans un cv ou une recette de cuisine\n",
    "\n",
    "l'objectif du ner c'est identifier toutes les mentions textuelles des entités nommées. On peut le faire en 2 temps : on identifie les limites du NE et ensuite son type.\n",
    "\n",
    "On parle alors d'apprche basé sur des classificateurs. Cela signifie que l'on utilise des techniques informatiques pour enseigner à un ordinateur à identifier automatiquement les entités nommées dans le texte.\n",
    "\n",
    "Exemple : on entraine un modele à reconnaitre differents types d'entités comme personne ou lieu. On lui montre alors de nombreux exemples de textes contenant ces entités déjà étiquetées\n",
    "\n",
    "Un tagger/etiqueteur va etre alors un composanr qui va attribuer des labels à chaque mot dans une phrase et du coup indiquer s'il appartient ou non. Dans le cas échéant on a determine le type d'entité.\n",
    "\n",
    "\n",
    "On utilise la convention \"IOB\", \"Inside-Outside-Beginning\". Elle va nous permettre d'étiqueter les mots dans un texte. Elle permet de marquer les mots qui font partie d'une entité, ceux qui commencent une nouvelle entité et ceux qui ne font pas partie d'une entité.\n",
    "\n",
    "\n",
    "Exemple :  \"Barack Obama a fondé la Obama Foundation en 2014 à Chicago.\"\n",
    "\n",
    "\"Barack\" : B-PER (Beginning Person)\n",
    "\"Obama\" : I-PER (Inside Person)\n",
    "\"a\" : O (Outside, pas une entité)\n",
    "\"fondé\" : O\n",
    "\"la\" : O\n",
    "\"Obama\" : B-ORG (Beginning Organization)\n",
    "\"Foundation\" : I-ORG (Inside Organization)\n",
    "\"en\" : O\n",
    "\"2014\" : B-DATE (Beginning Date)\n",
    "\"à\" : O\n",
    "\"Chicago\" : B-LOC (Beginning Location)\n",
    "\".\" : O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différentes méthodes/approches afin de réaliser une NER : \n",
    "\n",
    "- de manière classique :\n",
    " \n",
    "    Comme dans la peau d'un détective qui essaie de trouver des indices dans une histoires pour identifier des personnages, des endroits et des choses spéciales\n",
    "\n",
    "    C'est la même sauf qu'au lieu dêtre un detective le programme informatique va alors lire le texte et essayer de repérer les mots spéciaux genre lieux, marques, noms d'entreprises, personnes\n",
    "\n",
    "    Ce programme informatique a une liste de règles préétablies, un peu comme des instructions, pour reconnaître ces noms spéciaux. Les règles préétablies sont généralement créées par des linguistes, des experts en traitement du langage naturel ou des spécialistes du domaine. Ils examinent de nombreux textes pour identifier des modèles et des caractéristiques qui peuvent indiquer la présence d'entités nommées.\n",
    "\n",
    "    Inconvénients et Comparaison : Prenons l'exemple d'un jeu de devinettes. Imagine que tu es en train de jouer à deviner le nom d'une personne, mais les indices que tu reçois sont un peu rigides et basés sur des règles strictes. Par exemple, si quelqu'un dit \"le nom que je pense commence par la lettre B et a six lettres\", tu pourrais essayer des noms comme \"Banane\" ou \"Bateau\" simplement en suivant les règles, même si ce ne sont pas des noms de personnes.\n",
    "\n",
    "    Dans le contexte de la reconnaissance d'entités nommées, la méthode classique avec des règles préétablies est un peu comme cela. Elle peut parfois donner de bons résultats si les règles correspondent bien aux modèles dans le texte, mais elle peut aussi se tromper si le texte ne suit pas exactement les règles prévues. Par exemple, si le texte utilise un surnom pour une personne comme \"Bee\" au lieu de \"Barack\", la méthode classique pourrait ne pas le reconnaître comme un nom de personne.\n",
    "\n",
    "- grâce à ML :\n",
    "\n",
    "    - Classification Multiclasse :\n",
    "    \n",
    "        Imagine qu'on a un sac rempli de différentes sortes de fruits, comme des pommes, des bananes et des oranges. Chaque fruit a ses propres caractéristiques qui le rendent unique. Maintenant, on veux apprendre à un programme à reconnaître ces fruits. Une façon de faire cela est de montrer au programme différents fruits et lui dire à chaque fois de quel fruit il s'agit => CLASSIFICATION\n",
    "\n",
    "        La méthode de classification multiclasse traite la REN comme un jeu de devinettes. Les \"devinettes\" sont les différentes catégories d'entités nommées, comme les noms de personnes, les noms de lieux, etc. Le programme apprend à partir d'exemples comment identifier chaque type d'entité nommée.\n",
    "\n",
    "        Inconvénients : Imagine qu'on essaie de comprendre une histoire complexe en lisant simplement quelques morceaux de phrases. Tu pourrais ne pas saisir toute l'histoire ou les relations entre les personnages. De même, cette méthode peut avoir du mal à saisir le sens profond du texte et à suivre comment les entités nommées sont connectées dans une séquence.\n",
    "\n",
    "    - Modèle de Champ Aléatoire Conditionnel/Conditional Random Field (CRF) :\n",
    "\n",
    "        Pense au CRF comme à un puzzle dans lequel tu dois placer des morceaux de texte ensemble de manière à ce qu'ils aient du sens. Le modèle CRF est comme un ensemble de règles qui t'aide à placer ces morceaux ensemble de manière logique. Cela fonctionne bien lorsque le texte suit un ordre, comme les mots dans une phrase.\n",
    "\n",
    "        Dans la REN, chaque mot d'une phrase est comme un morceau de puzzle. Le modèle CRF aide à assembler ces morceaux en étiquetant chaque mot avec la bonne catégorie d'entité nommée. Il prend en compte les étiquettes précédentes pour s'assurer que tout s'emboîte correctement.\n",
    "\n",
    "        Inconvénients : Imagine que tu essaies de résoudre un puzzle sans voir tous les morceaux. Cela pourrait rendre certaines parties du puzzle difficiles à résoudre. De même, le modèle CRF a du mal à prédire certaines parties du texte qui viennent plus tard dans la séquence. De plus, ce modèle peut être complexe à construire et à former, ce qui le rend moins populaire dans l'industrie.\n",
    "\n",
    "- grâce au DL :\n",
    "\n",
    "    Imagine que tu lis une histoire dans un livre. Quand tu lis un mot, le mot précédent et le mot suivant peuvent souvent t'aider à comprendre le sens de ce mot. Les RNN bidirectionnels fonctionnent un peu comme si tu lisais une histoire dans les deux sens à la fois : du début à la fin et de la fin au début.\n",
    "\n",
    "    Maintenant, pense à une histoire où les personnages et les actions se développent au fil du temps. Les RNN bidirectionnels sont comme si tu lisais cette histoire tout en gardant à l'esprit les événements passés et futurs. Tu peux mieux comprendre les liens entre les personnages et les événements parce que tu as l'histoire complète dans ton esprit.\n",
    "\n",
    "    Dans le contexte de la reconnaissance d'entités nommées (REN), chaque mot d'une phrase est comme un mot dans l'histoire. Les RNN bidirectionnels analysent le texte en prenant en compte les mots qui viennent avant et après chaque mot. Cela permet de saisir les relations complexes entre les entités nommées et les autres mots dans le texte.\n",
    "\n",
    "    0Imagine que tu essayes de comprendre un jeu de mots croisés. Les indices dans les cases adjacentes peuvent t'aider à deviner les mots. De la même manière, les RNN bidirectionnels \"devinent\" quelles parties du texte sont des entités nommées en se basant sur le contexte avant et après chaque mot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy et BERT\n",
    "\n",
    "spaCy : spaCy est une bibliothèque de traitement du langage naturel très populaire et efficace. Elle permet de réaliser diverses tâches de traitement de texte, y compris la reconnaissance d'entités nommées (NER). spaCy offre des modèles pré-entraînés qui peuvent identifier automatiquement des entités telles que les noms de personnes, les noms de lieux, les dates, etc., dans le texte. Il permet également de créer et d'entraîner des modèles personnalisés pour des tâches spécifiques.\n",
    "\n",
    "BERT : BERT, qui signifie \"Bidirectional Encoder Representations from Transformers\", est un modèle de langage pré-entraîné développé par Google. BERT est un modèle basé sur les transformers, une architecture d'apprentissage profond qui excelle dans la compréhension contextuelle du langage. BERT est conçu pour capturer les relations entre les mots dans les deux directions, ce qui lui permet de saisir des informations contextuelles riches. Il a été utilisé pour améliorer considérablement les performances dans diverses tâches de traitement de texte, y compris la REN.\n",
    "\n",
    "CamemBERT : CamemBERT est une variante du modèle BERT spécialement adaptée à la langue française. Il a été pré-entraîné sur un grand corpus de textes en français pour mieux comprendre et représenter les spécificités de la langue française. CamemBERT est capable de saisir le sens et le contexte des mots en français, ce qui en fait un excellent choix pour les tâches de traitement de texte en français, y compris la reconnaissance d'entités nommées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking \n",
    "\n",
    "Pense à une histoire que tu es en train de lire. Parfois, tu peux remarquer que des mots sont regroupés ensemble pour former des parties importantes de l'histoire. Par exemple, dans la phrase \"Le chat noir dort paisiblement\", tu peux regrouper les mots \"le chat noir\" comme une unité qui décrit quelque chose.\n",
    "\n",
    "Dans le traitement du langage naturel (NLP), \"chunking\" est un processus similaire. C'est comme si nous prenions de petits morceaux d'informations, comme des mots, et les regroupions pour former des unités plus grandes et significatives. Cela nous aide à donner de la structure aux phrases en identifiant des groupes de mots qui ont une signification particulière\n",
    "\n",
    "Chunking en NLP est un processus de regrouper de petites informations en grandes unités. L'utilisation principale du chunking est de former des groupes de \"phrases nominales.\" Il est utilisé pour ajouter de la structure à la phrase en combinant l'étiquetage des parties du discours (POS tagging) avec des expressions régulières. Le groupe de mots résultant est appelé \"chunk.\" Il n'y a pas de règles pré-définies pour le chunking, mais nous pouvons les créer en fonction de nos besoins. Ainsi, si nous voulons regrouper uniquement les étiquettes 'NN' (noms), nous devons utiliser un modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALIZE DATA & PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(\"./bootstrap/corpus/ner_dataset.csv\", encoding = \"ISO-8859-1\", usecols=['Sentence #','Word','POS','Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_number = 0 # numéros de phrase\n",
    "for i, value in enumerate(data['Sentence #']): # parcourir chaque element de la colonne e 'Sentence #' \n",
    "    # i => index; value => valeur  \n",
    "    if pd.notnull(value): # si la valeur actuelle est le num de la phrase / n'est pas nulle\n",
    "        sentence_number += 1 # alors on += 1 sentence_number pour nvl phrase\n",
    "    data.loc[i, 'Sentence #'] = sentence_number # MAJ de l'index i avec la valeur actuelle de sentence_number\n",
    "data['Sentence #'].fillna(method='ffill', inplace=True)# on remplit les valeurs manquantes dans la colonne 'Sentence #' en utilisant la méthode 'ffill' (forward fill)\n",
    "data['Sentence #'] = data['Sentence #'].astype(int) # MAJ en entier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #           Word  POS Tag\n",
       "0           1      Thousands  NNS   O\n",
       "1           1             of   IN   O\n",
       "2           1  demonstrators  NNS   O\n",
       "3           1           have  VBP   O\n",
       "4           1        marched  VBN   O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          24\n",
      "1          24\n",
      "2          24\n",
      "3          24\n",
      "4          24\n",
      "           ..\n",
      "1048570     8\n",
      "1048571     8\n",
      "1048572     8\n",
      "1048573     8\n",
      "1048574     8\n",
      "Name: Sentence Length, Length: 1048575, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data['Sentence Length'] = data.groupby('Sentence #')['Word'].transform('count')\n",
    "print(data['Sentence Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Sentence Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #           Word  POS Tag  Sentence Length\n",
       "0           1      Thousands  NNS   O               24\n",
       "1           1             of   IN   O               24\n",
       "2           1  demonstrators  NNS   O               24\n",
       "3           1           have  VBP   O               24\n",
       "4           1        marched  VBN   O               24"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[O, O, O, O, O, O, -geoB, O, O, O, O, O, B-geo...</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[They, marched, from, the, Houses, of, Parliam...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...</td>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Police, put, the, number, of, marchers, at, 1...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[The, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...</td>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Word  \\\n",
       "Sentence #                                                      \n",
       "1           [Thousands, of, demonstrators, have, marched, ...   \n",
       "2           [Families, of, soldiers, killed, in, the, conf...   \n",
       "3           [They, marched, from, the, Houses, of, Parliam...   \n",
       "4           [Police, put, the, number, of, marchers, at, 1...   \n",
       "5           [The, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                                          Tag  \\\n",
       "Sentence #                                                      \n",
       "1           [O, O, O, O, O, O, -geoB, O, O, O, O, O, B-geo...   \n",
       "2           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3           [O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...   \n",
       "4               [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "5           [O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...   \n",
       "\n",
       "                                                     Sentence  \n",
       "Sentence #                                                     \n",
       "1           Thousands of demonstrators have marched throug...  \n",
       "2           Families of soldiers killed in the conflict jo...  \n",
       "3           They marched from the Houses of Parliament to ...  \n",
       "4           Police put the number of marchers at 10,000 wh...  \n",
       "5           The protest comes on the eve of the annual con...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.DataFrame(data.groupby('Sentence #')['Word'].apply(list))\n",
    "tags = data.groupby('Sentence #')['Tag'].apply(list)\n",
    "words['Tag'] = tags\n",
    "words['Sentence'] =  words['Word'].apply(lambda x: \" \".join(map(str, x)))\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_data = []\n",
    "\n",
    "for i, row in words.iterrows():\n",
    "    tokens = row[\"Word\"]\n",
    "    tags = row[\"Tag\"]\n",
    "    sentence = row[\"Sentence\"]\n",
    "    \n",
    "    entities = []\n",
    "    start_pos = 0  # Initialize the starting position\n",
    "    \n",
    "    for i in range(len(tags)):\n",
    "        if tags[i] != 'O':\n",
    "            end_pos = start_pos + len(tokens[i])  # Calculate end position\n",
    "            entities.append((start_pos, end_pos, tags[i]))\n",
    "            \n",
    "            start_pos = end_pos\n",
    "    \n",
    "    data_tuple = (sentence,  {'entities': entities})\n",
    "    spacy_data.append(data_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .',\n",
       "  {'entities': [(0, 6, '-geoB'), (6, 10, 'B-geo'), (10, 17, 'B-gpe')]}),\n",
       " ('Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"',\n",
       "  {'entities': [(0, 4, 'B-per')]}),\n",
       " ('They marched from the Houses of Parliament to a rally in Hyde Park .',\n",
       "  {'entities': [(0, 4, 'B-geo'), (4, 8, 'I-geo')]}),\n",
       " ('Police put the number of marchers at 10,000 while organizers claimed it was 1,00,000 .',\n",
       "  {'entities': []}),\n",
       " (\"The protest comes on the eve of the annual conference of Britain 's ruling Labor Party in the southern English seaside resort of Brighton .\",\n",
       "  {'entities': [(0, 7, 'B-geo'),\n",
       "    (7, 12, 'B-org'),\n",
       "    (12, 17, 'I-org'),\n",
       "    (17, 24, 'B-gpe'),\n",
       "    (24, 32, 'B-geo')]})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL WITH SPACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/usage/training#training-data\n",
    "https://towardsdatascience.com/train-ner-with-custom-training-data-using-spacy-525ce748fab7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from spacy.training.example import Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = spacy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "output_dir= Path(\"./ner_result/spacy\")\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    }
   ],
   "source": [
    "if model is not None:\n",
    "    nlp = spacy.load(model)  \n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  \n",
    "    print(\"Created blank 'en' model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the pipeline\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('ner')\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47959 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E989] `nlp.update()` was called with two positional arguments. This may be due to a backwards-incompatible change to the format of the training data in spaCy 3.0 onwards. The 'update' function should now be called with a batch of Example objects, instead of `(text, annotation)` tuples. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m losses \u001b[39m=\u001b[39m {}\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m text, annotations \u001b[39min\u001b[39;00m tqdm(TRAIN_DATA):\n\u001b[1;32m---> 12\u001b[0m     nlp\u001b[39m.\u001b[39;49mupdate(\n\u001b[0;32m     13\u001b[0m         [text],  \n\u001b[0;32m     14\u001b[0m         [annotations],  \n\u001b[0;32m     15\u001b[0m         drop\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,  \n\u001b[0;32m     16\u001b[0m         sgd\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m     17\u001b[0m         losses\u001b[39m=\u001b[39;49mlosses)\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(losses)\n",
      "File \u001b[1;32mc:\\Users\\ocean\\Documents\\Projects\\IA_T9\\IA\\moi_T-AIA-901_bootstrap\\venv\\lib\\site-packages\\spacy\\language.py:1164\u001b[0m, in \u001b[0;36mLanguage.update\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Update the models in the pipeline.\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \n\u001b[0;32m   1148\u001b[0m \u001b[39mexamples (Iterable[Example]): A batch of examples\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[39mDOCS: https://spacy.io/api/language#update\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m \u001b[39mif\u001b[39;00m _ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE989)\n\u001b[0;32m   1165\u001b[0m \u001b[39mif\u001b[39;00m losses \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     losses \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: [E989] `nlp.update()` was called with two positional arguments. This may be due to a backwards-incompatible change to the format of the training data in spaCy 3.0 onwards. The 'update' function should now be called with a batch of Example objects, instead of `(text, annotation)` tuples. "
     ]
    }
   ],
   "source": [
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()\n",
    "# doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.blank(\"en\")\n",
    "\n",
    "# training_data = spacy_data\n",
    "# print(training_data)\n",
    "\n",
    "# db = DocBin()\n",
    "# for text, annotations in training_data:\n",
    "#     doc = nlp.make_doc(text)  # Create a Doc object without processing\n",
    "#     ents = []\n",
    "#     for start, end, label in annotations:\n",
    "#         print(\"Creating entity:\", start, end, label)\n",
    "#         span = doc.char_span(start, end, label=label)\n",
    "#         if span is not None:  # Check if span is None before appending\n",
    "#             ents.append(span)\n",
    "\n",
    "#     doc.ents = ents\n",
    "#     db.add(doc)\n",
    "# db.to_disk(\"./train.spacy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
